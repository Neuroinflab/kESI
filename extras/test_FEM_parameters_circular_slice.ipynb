{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test how mesh granularity and element degree affect error of FEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import FEM.fem_slice_point_new as fspn\n",
    "import FEM.fem_common as fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We choose $N$ random points in the region of interest (here cube with edges of 0.3 mm length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 3e-4\n",
    "GEOMETRY = 'circular_slice'\n",
    "PROPERTIES = 'FEM/model_properties/circular_slice.ini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MESHES = ['coarsest',\n",
    "          'coarser',\n",
    "          'coarse',\n",
    "          'normal',\n",
    "          'fine',\n",
    "          'finer',\n",
    "          'finest',\n",
    "          ]\n",
    "\n",
    "DEGREES = [1, 2, 3]\n",
    "\n",
    "CONFIGS = list(itertools.product(MESHES, DEGREES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MESH_DIR = 'FEM/meshes/meshes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = os.path.join('test_FEM_parameters',\n",
    "                          GEOMETRY)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POINTS_FILE = os.path.join(RESULT_DIR,\n",
    "                           'point.csv')\n",
    "\n",
    "if os.path.exists(POINTS_FILE):\n",
    "    print(POINTS_FILE, 'found')\n",
    "    POINTS = pd.read_csv(POINTS_FILE,\n",
    "                         index_col=0)\n",
    "\n",
    "else:\n",
    "    random.seed(42)\n",
    "    POINTS = np.full((N, 3), np.nan)\n",
    "\n",
    "    for i in range(N):\n",
    "        x = random.uniform(-H/2, H/2)\n",
    "        y = random.uniform(-H/2, H/2)\n",
    "        z = random.uniform(0, H)\n",
    "        POINTS[i, :] = x, y, z\n",
    "\n",
    "    POINTS = pd.DataFrame(POINTS, columns=['X', 'Y', 'Z'])\n",
    "    POINTS.to_csv(POINTS_FILE,\n",
    "                  index_label='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points are saved so any further calculations may reuse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEM benchmarking\n",
    "\n",
    "For every granularity and degree configuration, for every of $N$ points, we use FEM to calculate appropriate correction for kCSD-like approximation of 1A point source.  The correction is probed at $N$ points and the $N\\times{}N$ values are saved for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_time = fc.fc.Stopwatch()\n",
    "total_solving_time = fc.fc.Stopwatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mesh, degree in CONFIGS:\n",
    "    print(mesh, degree)\n",
    "    result_file = os.path.join(RESULT_DIR,\n",
    "                               f'{mesh}_{degree}.csv')\n",
    "    \n",
    "    if os.path.exists(result_file):\n",
    "        print(' ', result_file, 'found')\n",
    "        continue\n",
    "    \n",
    "    DF = []\n",
    "    with setup_time:\n",
    "        function_manager = fc.FunctionManager(os.path.join(MESH_DIR,\n",
    "                                                           GEOMETRY,\n",
    "                                                           f'{mesh}.xdmf'),\n",
    "                                              degree,\n",
    "                                              'CG')\n",
    "        fem = fspn.SlicePointSourcePotentialFEM(function_manager,\n",
    "                                                PROPERTIES)\n",
    "\n",
    "    for src, SRC in POINTS.iterrows():\n",
    "        print(mesh, degree, src)\n",
    "        with total_solving_time:\n",
    "            potential_corr = fem.correction_potential(SRC.X, SRC.Y, SRC.Z)\n",
    "            \n",
    "        for dst, DST in POINTS.iterrows():\n",
    "            DF.append({\n",
    "                'SRC': src,\n",
    "                'DST': dst,\n",
    "                'CORR': potential_corr(DST.X, DST.Y, DST.Z),\n",
    "                'SOLVING_TIME': float(total_solving_time),\n",
    "                'SETUP_TIME': float(setup_time)\n",
    "            })\n",
    "            \n",
    "    DF = pd.DataFrame(DF)\n",
    "    DF.to_csv(result_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from local import cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import operator\n",
    "from kesi import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(PROPERTIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICE_CONDUCTIVITY = config.getfloat('slice', 'conductivity')\n",
    "SALINE_CONDUCTIVITY = config.getfloat('saline', 'conductivity')\n",
    "GLASS_CONDUCTIVITY = 0.0\n",
    "\n",
    "MOI_N = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth correction\n",
    "\n",
    "As the circular slice geometry may be approximated by an \"infinite slice\" geometry, it is prossible to calculate true values of correction with method of images (MOI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WTG = float(SLICE_CONDUCTIVITY - GLASS_CONDUCTIVITY) / (SLICE_CONDUCTIVITY + GLASS_CONDUCTIVITY)\n",
    "WTS = float(SLICE_CONDUCTIVITY - SALINE_CONDUCTIVITY) / (SLICE_CONDUCTIVITY + SALINE_CONDUCTIVITY)\n",
    "\n",
    "GT_CORRECTION = np.full((N, N), np.nan)\n",
    "\n",
    "for src, SRC in POINTS.iterrows():\n",
    "    print(src)\n",
    "    weights = []\n",
    "    sources = []\n",
    "    for i in range(MOI_N):\n",
    "        weights.append(WTG**i * WTS**(i+1))\n",
    "        sources.append(common.PointSource(SRC.X,\n",
    "                                          SRC.Y,\n",
    "                                          2 * (i + 1) * H - SRC.Z,\n",
    "                                          conductivity=SLICE_CONDUCTIVITY))\n",
    "        weights.append(WTG**(i+1) * WTS**i)\n",
    "        sources.append(common.PointSource(SRC.X,\n",
    "                                          SRC.Y,\n",
    "                                          -2 * i * H - SRC.Z,\n",
    "                                          conductivity=SLICE_CONDUCTIVITY))\n",
    "\n",
    "    for i in range(1, MOI_N + 1):\n",
    "        weights.append((WTG * WTS)**i)\n",
    "        sources.append(common.PointSource(SRC.X,\n",
    "                                          SRC.Y,\n",
    "                                          SRC.Z + 2 * i * H,\n",
    "                                          conductivity=SLICE_CONDUCTIVITY))\n",
    "        weights.append((WTG * WTS)**i)\n",
    "        sources.append(common.PointSource(SRC.X,\n",
    "                                          SRC.Y,\n",
    "                                          SRC.Z - 2 * i * H,\n",
    "                                          conductivity=SLICE_CONDUCTIVITY))\n",
    "\n",
    "#     positive = [(w, s) for w, s in zip(weights, sources) if w > 0]\n",
    "#     positive.sort(key=operator.itemgetter(0), reverse=False)\n",
    "#     negative = [(w, s) for w, s in zip(weights, sources) if w < 0]\n",
    "#     negative.sort(key=operator.itemgetter(0), reverse=True)\n",
    "\n",
    "    for dst, DST in POINTS.iterrows():\n",
    "        values = [w * s.potential(DST.X, DST.Y, DST.Z)\n",
    "                  for w, s in zip(weights, sources)]\n",
    "        positive = sorted([v for v in values if v > 0])\n",
    "        negative = sorted([-v for v in values if v < 0])\n",
    "        GT_CORRECTION[src, dst] = sum(positive) - sum(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate MOI we calculate the maximal reciprocity error\n",
    "(due to reciprocity the ideal `GT_CORRECTION` matrix is\n",
    "symmetrical).  We also estimate order of magnitude of the\n",
    "off-diagonal elements.\n",
    "\n",
    "> The diagonal elements are insignificant, as the base potential\n",
    "> ($V_{base} \\propto \\frac{1}{r}$) is a singularity for $r = 0$.\n",
    "> Moreover, for points near to the medium interface the amplitude\n",
    "> of the correction potential is high at the point source location\n",
    "> due to the proximity of its (source's) image reflected by\n",
    "> the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OFF_DIAGONAL_IDX = ~np.eye(N, dtype=bool)\n",
    "\n",
    "print('Maximal reciprocity error:', abs(GT_CORRECTION - GT_CORRECTION.T).max())\n",
    "_OFF_DIAGONAL = GT_CORRECTION[OFF_DIAGONAL_IDX]\n",
    "print('Linf:', abs(_OFF_DIAGONAL).max())\n",
    "print('L2:', np.sqrt(np.square(_OFF_DIAGONAL).mean()))\n",
    "print('L1:', abs(_OFF_DIAGONAL).mean())\n",
    "print('Median absolute value:', np.median(abs(_OFF_DIAGONAL)))\n",
    "print('min, med, max:', _OFF_DIAGONAL.min(), np.median(_OFF_DIAGONAL), _OFF_DIAGONAL.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base potential\n",
    "\n",
    "To enable estimation of the relative error of potential (leadfield due to reciprocity) rather than correction (which may inflate the error where correction is small), we calculate the `BASE_POTENTIAL` matrix and then the `GT_POTENTIAL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_POTENTIAL = np.full((N, N), np.nan)\n",
    "\n",
    "for src, SRC in POINTS.iterrows():\n",
    "    _src = common.PointSource(SRC.X,\n",
    "                              SRC.Y,\n",
    "                              SRC.Z,\n",
    "                              conductivity=SLICE_CONDUCTIVITY)\n",
    "    for dst, DST in POINTS.iterrows():\n",
    "        BASE_POTENTIAL[src, dst] = _src.potential(DST.X, DST.Y, DST.Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximal reciprocity error:', abs(BASE_POTENTIAL - BASE_POTENTIAL.T)[OFF_DIAGONAL_IDX].max())\n",
    "_OFF_DIAGONAL = BASE_POTENTIAL[OFF_DIAGONAL_IDX]\n",
    "print('Linf:', abs(_OFF_DIAGONAL).max())\n",
    "print('L2:', np.sqrt(np.square(_OFF_DIAGONAL).mean()))\n",
    "print('L1:', abs(_OFF_DIAGONAL).mean())\n",
    "print('Median absolute value:', np.median(abs(_OFF_DIAGONAL)))\n",
    "print('min, med, max:', _OFF_DIAGONAL.min(), np.median(_OFF_DIAGONAL), _OFF_DIAGONAL.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_POTENTIAL = BASE_POTENTIAL + GT_CORRECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximal reciprocity error:', abs(GT_POTENTIAL - GT_POTENTIAL.T)[OFF_DIAGONAL_IDX].max())\n",
    "_OFF_DIAGONAL = GT_POTENTIAL[OFF_DIAGONAL_IDX]\n",
    "print('Linf:', abs(_OFF_DIAGONAL).max())\n",
    "print('L2:', np.sqrt(np.square(_OFF_DIAGONAL).mean()))\n",
    "print('L1:', abs(_OFF_DIAGONAL).mean())\n",
    "print('Median absolute value:', np.median(abs(_OFF_DIAGONAL)))\n",
    "print('min, med, max:', _OFF_DIAGONAL.min(), np.median(_OFF_DIAGONAL), _OFF_DIAGONAL.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading correction matrices\n",
    "\n",
    "We read the saved correction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "corrections = []\n",
    "\n",
    "for mesh, degree in CONFIGS:\n",
    "    print(mesh, degree)\n",
    "    result_file = os.path.join(RESULT_DIR,\n",
    "                               f'{mesh}_{degree}.csv')\n",
    "    if not os.path.exists(result_file):\n",
    "        print(' not found, skipping')\n",
    "        continue\n",
    "\n",
    "    labels.append(f'{mesh} {degree}')\n",
    "\n",
    "    DF = pd.read_csv(result_file)\n",
    "    CORR = np.full((N, N), np.nan)\n",
    "    \n",
    "    for row in DF.itertuples():\n",
    "        CORR[row.SRC, row.DST] = row.CORR\n",
    "        \n",
    "    corrections.append(CORR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reciprocity validation\n",
    "\n",
    "The first test is a simple verification, whether the reciprocity is held by the result.\n",
    "Reciprocity is crucial, as whole kESI optimization relies on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reciprocity_errors = [A - A.T for A in corrections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Reciprocity errors [V]')\n",
    "plt.yscale('symlog')\n",
    "_ = plt.boxplot([np.ravel(A) for A in reciprocity_errors],\n",
    "                labels=labels)\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Module of reciprocity errors [V]')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "_ = plt.violinplot([A[A > 0] for A in reciprocity_errors])\n",
    "_ = plt.boxplot([A[A > 0] for A in reciprocity_errors],\n",
    "                labels=labels)\n",
    "# _ = plt.violinplot([A[A > 0] for A in reciprocity_errors])\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Module of reciprocity errors [%]')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "_ = plt.violinplot([100 * (A / GT_POTENTIAL)[A > 0] for A in reciprocity_errors])\n",
    "_ = plt.boxplot([100 * (A / GT_POTENTIAL)[A > 0] for A in reciprocity_errors],\n",
    "                labels=labels)\n",
    "# _ = plt.violinplot([A[A > 0] for A in reciprocity_errors])\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gt_errors = [A - GT_CORRECTION for A in corrections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_relative_errors = [(A / GT_POTENTIAL)[OFF_DIAGONAL_IDX] for A in gt_errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Errors [V]')\n",
    "plt.yscale('symlog')\n",
    "_ = plt.boxplot([np.ravel(A) for A in gt_errors],\n",
    "                labels=labels)\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Module of errors [V]')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "_ = plt.boxplot([abs(np.ravel(A)) for A in gt_errors],\n",
    "                labels=labels)\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Errors (diagonal excluded) [V]')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "_ = plt.boxplot([abs(A[OFF_DIAGONAL_IDX]) for A in gt_errors],\n",
    "                labels=labels)\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Modules of errors [%]')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "_ = plt.boxplot([100 * abs(A) for A in gt_relative_errors],\n",
    "                labels=labels)\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_error_L1 = np.array([abs(_DIFF[OFF_DIAGONAL_IDX]).mean() for _DIFF in gt_errors])\n",
    "true_error_L2 = np.array([np.sqrt(np.square(_DIFF[OFF_DIAGONAL_IDX]).mean()) for _DIFF in gt_errors])\n",
    "true_error_Linf = np.array([abs(_DIFF[OFF_DIAGONAL_IDX]).max() for _DIFF in gt_errors])\n",
    "true_error_bias = np.array([_DIFF[OFF_DIAGONAL_IDX].mean() for _DIFF in gt_errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_relative_error_L1 = np.array([abs(_DIFF).mean() for _DIFF in gt_relative_errors])\n",
    "true_relative_error_L2 = np.array([np.sqrt(np.square(_DIFF).mean()) for _DIFF in gt_relative_errors])\n",
    "true_relative_error_Linf = np.array([abs(_DIFF).max() for _DIFF in gt_relative_errors])\n",
    "true_relative_error_bias = np.array([_DIFF.mean() for _DIFF in gt_relative_errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Errors (diagonal excluded) [V]')\n",
    "\n",
    "plt.plot(true_error_L1, label='L1', marker='o')\n",
    "plt.plot(true_error_L2, label='L2', marker='+')\n",
    "plt.plot(true_error_Linf, label='L\\u221e')\n",
    "plt.plot(true_error_bias, label='bias')\n",
    "plt.yscale('symlog', linthresh=0.1)\n",
    "plt.xticks(range(len(labels)), labels)\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Errors [%]')\n",
    "\n",
    "plt.plot(true_relative_error_L1 * 100, label='L1', marker='o')\n",
    "plt.plot(true_relative_error_L2 * 100, label='L2', marker='+')\n",
    "plt.plot(true_relative_error_Linf * 100, label='L\\u221e')\n",
    "plt.plot(true_relative_error_bias * 100, label='bias')\n",
    "plt.yscale('symlog', linthresh=0.1)\n",
    "plt.xticks(range(len(labels)), labels)\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence validation\n",
    "\n",
    "As for spherical geometries no exact values of correction are known,\n",
    "we have to approximate the exact correction as an average of its approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We approximate exact solution as average of the most advanced FEM configurations\n",
    "# (in terms of mesh density and element degree)\n",
    "\n",
    "_corrections = dict(zip(labels, corrections))\n",
    "\n",
    "AVG = 0.5 * (_corrections['finer 3'] + _corrections['finest 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The algorithm below approximates the exact correction\n",
    "# # (`_AVG`) as an average of a set of its numeric approximations.\n",
    "# # Aproximation errors are then estimated with `_AVG`\n",
    "# # and the worst numeric approximation is excluded from the set.\n",
    "# # The `_AVG` is reevaluated and the whole process is repeated\n",
    "# # as long as only 2 approximations are left in the set\n",
    "\n",
    "# _corrections = dict(zip(labels, corrections))\n",
    "\n",
    "# removed = []\n",
    "# removal_score = []\n",
    "# scores = []\n",
    "\n",
    "# _score = 1\n",
    "# while len(_corrections) > 2 and _score:\n",
    "#     _AVG = sum(_corrections.values()) / len(_corrections)\n",
    "#     _EST_POT = BASE_POTENTIAL + _AVG\n",
    "#     _score = 0\n",
    "#     scores.append([])\n",
    "#     for _k, _CORR in _corrections.items():\n",
    "# #         _s = np.sqrt(np.square(((_AVG - _CORR))[OFF_DIAGONAL_IDX]).mean())\n",
    "#         _s = np.sqrt(np.square(((_AVG - _CORR) / _EST_POT)[OFF_DIAGONAL_IDX]).mean())\n",
    "#         scores[-1].append(_s)\n",
    "#         if _s > _score:\n",
    "#             _score = _s\n",
    "#             _key = _k\n",
    "            \n",
    "#     removed.append(_key)\n",
    "#     removal_score.append(_score)\n",
    "#     del _corrections[_key]\n",
    "\n",
    "# AVG = sum(_corrections.values()) / len(_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _EST_POT = BASE_POTENTIAL + AVG\n",
    "\n",
    "# for k, v in zip(removed, removal_score):\n",
    "#     print(f'  {k}\\t{v}')\n",
    "    \n",
    "# for k, v in zip(labels, corrections):\n",
    "#     if k in removed:\n",
    "#         continue\n",
    "    \n",
    "#     minimal_score = np.sqrt(np.square(((v - AVG) / _EST_POT)[OFF_DIAGONAL_IDX]).mean())\n",
    "#     print(f'> {k}\\t{minimal_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(removal_score)\n",
    "# plt.axhline(minimal_score)\n",
    "# plt.yscale('log')\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, _scores in enumerate(scores):\n",
    "#     plt.scatter([i] * len(_scores), _scores)\n",
    "# plt.yscale('linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, _scores in enumerate(scores):\n",
    "#     plt.scatter([i] * len(_scores), _scores)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = [_CORR - AVG for _CORR in corrections]\n",
    "error_L1 = np.array([abs(_DIFF[OFF_DIAGONAL_IDX]).mean() for _DIFF in diffs])\n",
    "error_L2 = np.array([np.sqrt(np.square(_DIFF[OFF_DIAGONAL_IDX]).mean()) for _DIFF in diffs])\n",
    "error_Linf = np.array([abs(_DIFF[OFF_DIAGONAL_IDX]).max() for _DIFF in diffs])\n",
    "error_bias = np.array([_DIFF[OFF_DIAGONAL_IDX].mean() for _DIFF in diffs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_relative = [_DIFF / (AVG + BASE_POTENTIAL) for _DIFF in diffs]\n",
    "error_relative_L1 = np.array([abs(_DIFF[OFF_DIAGONAL_IDX]).mean() for _DIFF in diffs_relative])\n",
    "error_relative_L2 = np.array([np.sqrt(np.square(_DIFF[OFF_DIAGONAL_IDX]).mean()) for _DIFF in diffs_relative])\n",
    "error_relative_Linf = np.array([abs(_DIFF[OFF_DIAGONAL_IDX]).max() for _DIFF in diffs_relative])\n",
    "error_relative_bias = np.array([_DIFF[OFF_DIAGONAL_IDX].mean() for _DIFF in diffs_relative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Modulus of convergence errors [V]')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "_ = plt.boxplot([abs(np.ravel(A)) for A in diffs],\n",
    "                labels=labels)\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Modulus of convergence errors (diagonal excluded) [V]')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "_ = plt.boxplot([abs(A[OFF_DIAGONAL_IDX]) for A in diffs],\n",
    "                labels=labels)\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Modulus of convergence errors (diagonal included) [%]')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "_ = plt.boxplot([100 * abs(np.ravel(A)) for A in diffs_relative],\n",
    "                labels=labels)\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Modulus of convergence errors (diagonal excluded) [%]')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "_ = plt.boxplot([100 * abs(A[OFF_DIAGONAL_IDX]) for A in diffs_relative],\n",
    "                labels=labels)\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Convergence errors [V]')\n",
    "\n",
    "plt.plot(error_L1, label='L1')\n",
    "plt.plot(error_L2, label='L2')\n",
    "plt.plot(error_Linf, label='L\\u221e')\n",
    "plt.plot(error_bias, label='bias')\n",
    "plt.yscale('symlog', linthresh=0.1)\n",
    "plt.xticks(range(len(labels)), labels)\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark of error estimation\n",
    "\n",
    "Since for slice geometry the exact correction is known, we can benchmark\n",
    "our method of its estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Convergence and true errors [V]')\n",
    "\n",
    "plt.plot(true_error_L1,\n",
    "         color=plt.plot(error_L1, label='L1')[0].get_color(),\n",
    "         ls='--')\n",
    "\n",
    "plt.plot(true_error_L2,\n",
    "         color=plt.plot(error_L2, label='L2')[0].get_color(),\n",
    "         ls='--')\n",
    "plt.plot(true_error_Linf,\n",
    "         color=plt.plot(error_Linf, label='L\\u221e')[0].get_color(),\n",
    "         ls='--')\n",
    "plt.plot(true_error_bias,\n",
    "         color=plt.plot(error_bias, label='bias')[0].get_color(),\n",
    "         ls='--')\n",
    "plt.yscale('symlog', linthresh=0.1)\n",
    "plt.xticks(range(len(labels)), labels)\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Convergence and true errors [%]')\n",
    "\n",
    "plt.plot(true_relative_error_L1 * 100,\n",
    "         color=plt.plot(error_relative_L1 * 100, label='L1')[0].get_color(),\n",
    "         ls='--')\n",
    "\n",
    "plt.plot(true_relative_error_L2 * 100,\n",
    "         color=plt.plot(error_relative_L2 * 100, label='L2')[0].get_color(),\n",
    "         ls='--')\n",
    "plt.plot(true_relative_error_Linf * 100,\n",
    "         color=plt.plot(error_relative_Linf * 100, label='L\\u221e')[0].get_color(),\n",
    "         ls='--')\n",
    "plt.plot(true_relative_error_bias * 100,\n",
    "         color=plt.plot(error_relative_bias * 100, label='bias')[0].get_color(),\n",
    "         ls='--')\n",
    "plt.yscale('symlog', linthresh=1)\n",
    "plt.xticks(range(len(labels)), labels)\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "_ = plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.title('Errors vs estimates')\n",
    "plt.xlabel('Error [V]')\n",
    "plt.ylabel('Estimate [V]')\n",
    "\n",
    "\n",
    "for _TRUE, _CONV in zip(gt_errors, diffs):\n",
    "    plt.scatter(np.ravel(_TRUE), np.ravel(_CONV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.title('Errors vs estimates')\n",
    "plt.xlabel('Error [V]')\n",
    "plt.ylabel('Estimate [V]')\n",
    "\n",
    "plt.xscale('symlog')\n",
    "plt.yscale('symlog')\n",
    "\n",
    "for _TRUE, _CONV in zip(gt_errors, diffs):\n",
    "    plt.scatter(np.ravel(_TRUE), np.ravel(_CONV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.title('Errors vs estimates (diagonal removed)')\n",
    "plt.xlabel('Error')\n",
    "plt.ylabel('Estimate')\n",
    "\n",
    "for _TRUE, _CONV in zip(gt_errors, diffs):\n",
    "    plt.scatter(_TRUE[OFF_DIAGONAL_IDX], _CONV[OFF_DIAGONAL_IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.title('Errors vs estimates (diagonal removed)')\n",
    "plt.xlabel('Error [V]')\n",
    "plt.ylabel('Estimate [V]')\n",
    "\n",
    "plt.xscale('symlog')\n",
    "plt.yscale('symlog')\n",
    "\n",
    "plt.plot([-1e4, 1e4], [-1e4, 1e4], ls=':', color='k')\n",
    "plt.plot([-1e4, 1e4], [1e4, -1e4], ls=':', color='k')\n",
    "plt.axvline(0, ls=':', color='k')\n",
    "plt.axhline(0, ls=':', color='k')\n",
    "plt.axvline(1, ls=':', color='k')\n",
    "plt.axhline(1, ls=':', color='k')\n",
    "plt.axvline(-1, ls=':', color='k')\n",
    "plt.axhline(-1, ls=':', color='k')\n",
    "\n",
    "for _TRUE, _CONV in zip(gt_errors, diffs):\n",
    "    plt.scatter(_TRUE[OFF_DIAGONAL_IDX], _CONV[OFF_DIAGONAL_IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, _TRUE, _CONV in zip(labels, gt_errors, diffs):\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.title(f'{name} (diagonal removed)')\n",
    "    plt.xlabel('Error [V]')\n",
    "    plt.ylabel('Estimate [V]')\n",
    "\n",
    "    plt.xscale('symlog')\n",
    "    plt.yscale('symlog')\n",
    "    \n",
    "    plt.xlim(-1e4, 1e4)\n",
    "    plt.ylim(-1e4, 1e4)\n",
    "    plt.plot([-1e4, 1e4], [-1e4, 1e4], ls=':', color='k')\n",
    "    plt.plot([-1e4, 1e4], [1e4, -1e4], ls=':', color='k')\n",
    "    plt.axvline(0, ls=':', color='k')\n",
    "    plt.axhline(0, ls=':', color='k')\n",
    "    plt.axvline(1, ls=':', color='k')\n",
    "    plt.axhline(1, ls=':', color='k')\n",
    "    plt.axvline(-1, ls=':', color='k')\n",
    "    plt.axhline(-1, ls=':', color='k')\n",
    "    plt.scatter(_TRUE[OFF_DIAGONAL_IDX], _CONV[OFF_DIAGONAL_IDX], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.title('Errors vs estimates (diagonal removed)')\n",
    "plt.xlabel('Error [%]')\n",
    "plt.ylabel('Estimate [%]')\n",
    "\n",
    "plt.xscale('symlog')\n",
    "plt.yscale('symlog')\n",
    "\n",
    "plt.plot([-1e3, 1e3], [-1e3, 1e3], ls=':', color='k')\n",
    "plt.plot([-1e3, 1e3], [1e3, -1e3], ls=':', color='k')\n",
    "plt.axvline(0, ls=':', color='k')\n",
    "plt.axhline(0, ls=':', color='k')\n",
    "plt.axvline(1, ls=':', color='k')\n",
    "plt.axhline(1, ls=':', color='k')\n",
    "plt.axvline(-1, ls=':', color='k')\n",
    "plt.axhline(-1, ls=':', color='k')\n",
    "\n",
    "for _TRUE, _CONV in zip(gt_relative_errors, diffs_relative):\n",
    "    plt.scatter(100 * _TRUE, 100 * _CONV[OFF_DIAGONAL_IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for name, _TRUE, _CONV in zip(labels, gt_relative_errors, diffs_relative):\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.title(f'{name} (diagonal removed)')\n",
    "    plt.xlabel('Error [%]')\n",
    "    plt.ylabel('Estimate [%]')\n",
    "\n",
    "    plt.xscale('symlog')\n",
    "    plt.yscale('symlog')\n",
    "    \n",
    "    plt.xlim(-1e3, 1e3)\n",
    "    plt.ylim(-1e3, 1e3)\n",
    "    plt.plot([-1e3, 1e3], [-1e3, 1e3], ls=':', color='k')\n",
    "    plt.plot([-1e3, 1e3], [1e3, -1e3], ls=':', color='k')\n",
    "    plt.axvline(0, ls=':', color='k')\n",
    "    plt.axhline(0, ls=':', color='k')\n",
    "    plt.axvline(1, ls=':', color='k')\n",
    "    plt.axhline(1, ls=':', color='k')\n",
    "    plt.axvline(-1, ls=':', color='k')\n",
    "    plt.axhline(-1, ls=':', color='k')\n",
    "    plt.scatter(100 * _TRUE, 100 * _CONV[OFF_DIAGONAL_IDX], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kesi3.7]",
   "language": "python",
   "name": "conda-env-kesi3.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
